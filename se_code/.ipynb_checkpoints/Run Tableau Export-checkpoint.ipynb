{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Lumi data...\n",
      "Creating doc table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/se_code-0.1-py3.6.egg/se_code/fuzzy_logic.py:38: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file doc_table.csv.\n",
      "Writing to file xref_table.csv.\n",
      "Creating themes table...\n",
      "Writing to file themes_table.csv.\n",
      "Creating subset key terms table...\n",
      "Writing to file skt_table.csv.\n",
      "Writing to file drivers_table.csv.\n",
      "Writing to file trends_table.csv.\n",
      "Warning: No data to write to trends_table.csv.\n",
      "Writing to file trendingterms_table.csv.\n",
      "Warning: No data to write to trendingterms_table.csv.\n"
     ]
    }
   ],
   "source": [
    "%run tableau_export.py r85b548r prnzh4tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_into_excel(folder_name, file_name):\n",
    "    wb = xlwt.Workbook()\n",
    "    index = 0\n",
    "    for filename in os.listdir(\"~Documents/My_Tableau_Repository/Datasources/\" + folder_name +\"/*.csv\"):\n",
    "#    for filename in os.listdir(\"c:/xxx/*.csv\"):\n",
    "        (f_path, f_name) = os.path.split(filename)\n",
    "        (f_short_name, f_extension) = os.path.splitext(f_name)\n",
    "        ws = wb.add_sheet(f_short_name)\n",
    "        spamReader = csv.reader(open(filename, 'rb'))\n",
    "        for rowx, row in enumerate(spamReader):\n",
    "            for colx, value in enumerate(row):\n",
    "                ws.write(rowx, colx, value)\n",
    "        index += 1\n",
    "    print(index)\n",
    "    wb.save(\"~Documents/My_Tableau_Repository/Datasources/\" + folder_name + '/' + file_name + \".xlsx\")\n",
    "#    wb.save(\"c:/xxx/compiled.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d9b475d14674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcsv_into_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LogMeIn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Compiled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-8f23fcdc4f9f>\u001b[0m in \u001b[0;36mcsv_into_excel\u001b[0;34m(folder_name, file_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#    wb.save(\"~Documents/My_Tableau_Repository/Datasources/\" + folder_name + '/' + file_name + \".xlsx\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"c:/xxx/compiled.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xlwt/Workbook.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename_or_stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompoundDoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXlsDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_biff_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xlwt/Workbook.py\u001b[0m in \u001b[0;36mget_biff_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0meof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__eof_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__worksheets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__active_sheet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0msheets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0msheet_biff_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "csv_into_excel('LogMeIn', 'Compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luminoso_api import LuminosoClient\n",
    "from pack64 import unpack64\n",
    "import run_voting_classifier # need accuracy/coverage chart\n",
    "from conjunctions_disjunctions import get_new_results\n",
    "from subset_key_terms import subset_key_terms\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import glob, xlwt, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client, docs, topics, terms, subsets, drivers, skt, themes = pull_lumi_data('d53m338v', 'pr5cxsm3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating doc table...\n",
      "dict_items([('__all__', 0), ('Locale', 1), ('Product', 2), ('Subject', 3)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/se_code-0.1-py3.6.egg/se_code/fuzzy_logic.py:38: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "doc_table, xref_table = create_doc_table(client, docs, subsets, themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_as(vector1, vector2):\n",
    "    return np.dot(unpack64(vector1), unpack64(vector2))\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_lumi_data(account, project, term_count=1000, interval='day', themes=7, theme_terms=4):\n",
    "\n",
    "    print('Extracting Lumi data...')\n",
    "    client = LuminosoClient.connect('/projects/{}/{}'.format(account, project))\n",
    "    subsets = client.get('subsets/stats')\n",
    "\n",
    "    docs = []\n",
    "    while True:\n",
    "        new_docs = client.get('docs', limit=25000, offset=len(docs))\n",
    "        if new_docs:\n",
    "            docs.extend(new_docs)\n",
    "        else:\n",
    "            break\n",
    "    drivers = list(set([key for d in docs for key in d['predict'].keys()]))\n",
    "\n",
    "    # See if any score drivers are present, if not, create some from subsets\n",
    "    if not any(drivers):\n",
    "        drivers = []\n",
    "        subset_headings = list(set([s['subset'].partition(':')[0] for s in subsets]))\n",
    "        for subset in subset_headings:\n",
    "            subset_values = [s['subset'].partition(':')[2] for s in subsets\n",
    "                             if s['subset'].partition(':')[0] == subset]\n",
    "            if all([is_number(v) for v in subset_values]):\n",
    "                drivers.append(subset)\n",
    "        if drivers:\n",
    "            add_score_drivers_to_project(client, docs, drivers)\n",
    "\n",
    "    topics = client.get('topics')\n",
    "    themes = client.get('/terms/clusters/', num_clusters=themes, num_cluster_terms=theme_terms)\n",
    "    terms = client.get('terms', limit=term_count)\n",
    "    terms_doc_count = client.get('terms/doc_counts', limit=term_count, format='json')\n",
    "    skt = subset_key_terms(client, 20)\n",
    "    return client, docs, topics, terms, subsets, drivers, skt, themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_doc_table(client, docs, subsets, themes):\n",
    "\n",
    "    print('Creating doc table...')\n",
    "    doc_table = []\n",
    "    xref_table = []\n",
    "    subset_headings = list(set([s['subset'].partition(':')[0] for s in subsets]))\n",
    "    subset_headings = {s: i for i, s in enumerate(subset_headings)}\n",
    "    print(subset_headings.items())\n",
    "    xref_table.extend([{'Header': 'Subset {}'.format(n), 'Name': h} for h,n in subset_headings.items()])\n",
    "\n",
    "    for i, theme in enumerate(themes):\n",
    "        search_terms = [t['text'] for t in theme['terms']]\n",
    "        theme['name'] = ', '.join(search_terms)[:-2]\n",
    "        theme['docs'] = get_new_results(client, search_terms, [], 'docs', 20, 'conjunction', False)\n",
    "        xref_table.append({'Header': 'Theme {}'.format(i), 'Name': theme['name']})\n",
    "\n",
    "    for doc in docs:\n",
    "        row = {}\n",
    "        row['doc_id'] = doc['_id']\n",
    "        row['doc_text'] = doc['text']\n",
    "        if 'date' in doc:\n",
    "            row['doc_date'] = doc['date']\n",
    "        else:\n",
    "            row['doc_date'] = 0\n",
    "        row.update({'Subset {}'.format(i): '' for i in range(len(subset_headings))})\n",
    "        row.update({'Subset {}_centrality'.format(i): 0 for i in range(len(subset_headings))})\n",
    "\n",
    "        for subset in doc['subsets']:\n",
    "            subset_partition = subset.partition(':')\n",
    "            if subset_partition[0] in subset_headings:\n",
    "                row['Subset {}'.format(subset_headings[subset_partition[0]])] = subset_partition[2]\n",
    "                row['Subset {}_centrality'.format(subset_headings[subset_partition[0]])] = get_as(doc['vector'],\n",
    "                    [s['mean'] for s in subsets if s['subset'] == subset][0])\n",
    "\n",
    "        for i, theme in enumerate(themes):\n",
    "            row['Theme {}'.format(i)] = 0\n",
    "            if doc['_id'] in [d['_id'] for d in theme['docs']]:\n",
    "                row['Theme {}'.format(i)] = [d['score'] for d in theme['docs'] if d['_id'] == doc['_id']][0]\n",
    "        doc_table.append(row)\n",
    "    return doc_table, xref_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_skt_table(client, skt):\n",
    "\n",
    "    print('Creating subset key terms table...')\n",
    "    terms = client.get('terms/doc_counts',\n",
    "                       terms=[t['term'] for _, t, _, _ in skt],\n",
    "                       format='json')\n",
    "    terms = {t['text']: t for t in terms}\n",
    "    skt_table = [{'term': t['text'],\n",
    "                  'subset': s.partition(':')[0],\n",
    "                  'value': s.partition(':')[2],\n",
    "                  'odds_ratio': o,\n",
    "                  'p_value': p,\n",
    "                  'exact_matches': terms[t['text']]['num_exact_matches'],\n",
    "                  'conceptual_matches': terms[t['text']]['num_related_matches']}\n",
    "                 for s, t, o, p in skt]\n",
    "    return skt_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_score_drivers_to_project(client, docs, drivers):\n",
    "    mod_docs = []\n",
    "    for doc in docs:\n",
    "        for subset_to_score in drivers:\n",
    "            if subset_to_score in [a.split(':')[0] for a in doc['subsets']]:\n",
    "                mod_docs.append({'_id': doc['_id'],\n",
    "                                 'predict': {subset_to_score: float([a for a in doc['subsets'] \n",
    "                                    if subset_to_score in a][0].split(':')[1])}})\n",
    "    client.put_data('docs', json.dumps(mod_docs), content_type='application/json')\n",
    "    client.post('docs/recalculate')\n",
    "\n",
    "    time_waiting = 0\n",
    "    while True:\n",
    "        if time_waiting%30 == 0:\n",
    "            if len(client.get()['running_jobs']) == 0:\n",
    "                break\n",
    "        sys.stderr.write('\\r\\tWaiting for recalculation ({}sec)'.format(time_waiting))\n",
    "        time.sleep(30)\n",
    "        time_waiting += 30\n",
    "    print('Done recalculating. Training...')\n",
    "    client.post('prediction/train')\n",
    "    print('Done training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_themes_table(client, themes):\n",
    "    print('Creating themes table...')\n",
    "    for i, theme in enumerate(themes):\n",
    "        search_terms = [t['text'] for t in theme['terms']]\n",
    "        theme['name'] = ', '.join(search_terms)\n",
    "        theme['id'] = i\n",
    "        theme['docs'] = sum([t['distinct_doc_count'] for t in theme['terms']])\n",
    "        del theme['terms']\n",
    "    return themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_drivers_table(client, drivers):\n",
    "    driver_table = []\n",
    "    for subset in drivers:\n",
    "        score_drivers = client.get('prediction/drivers', predictor_name=subset)\n",
    "        for driver in score_drivers['negative']:\n",
    "            row = {}\n",
    "            row['driver'] = driver['text']\n",
    "            row['subset'] = subset\n",
    "            row['impact'] = driver['regressor_dot']\n",
    "            row['score'] = driver['driver_score']\n",
    "    \n",
    "            # Use the driver term to find related documents\n",
    "            search_docs = client.get('docs/search', terms=[driver['term']], limit=500, exact_only=True)\n",
    "    \n",
    "            # Sort documents based on their association with the coefficient vector\n",
    "            for doc in search_docs['search_results']:\n",
    "                document = doc[0]['document']\n",
    "                document['driver_as'] = get_as(score_drivers['coefficient_vector'],document['vector'])\n",
    "\n",
    "            docs = sorted(search_docs['search_results'], key=lambda k: k[0]['document']['driver_as'])\n",
    "            row['example_doc'] = docs[0][0]['document']['text']\n",
    "            driver_table.append(row)\n",
    "        for driver in score_drivers['positive']:\n",
    "            row = {}\n",
    "            row['driver'] = driver['text']\n",
    "            row['subset'] = subset\n",
    "            row['impact'] = driver['regressor_dot']\n",
    "            row['score'] = driver['driver_score']\n",
    "\n",
    "            # Use the driver term to find related documents\n",
    "            search_docs = client.get('docs/search', terms=[driver['term']], limit=500, exact_only=True)\n",
    "\n",
    "            # Sort documents based on their association with the coefficient vector\n",
    "            for doc in search_docs['search_results']:\n",
    "                document = doc[0]['document']\n",
    "                document['driver_as'] = get_as(score_drivers['coefficient_vector'],document['vector'])\n",
    "\n",
    "            docs = sorted(search_docs['search_results'], key=lambda k: -k[0]['document']['driver_as'])\n",
    "            row['example_doc'] = docs[0][0]['document']['text']\n",
    "            driver_table.append(row)\n",
    "    return driver_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_trends_table(terms, topics, docs):\n",
    "    term_vecs = np.asarray([unpack64(t['vector']) for t in terms])\n",
    "    concept_list = [t['text'] for t in terms]\n",
    "\n",
    "    dated_docs = [d for d in docs if 'date' in d]\n",
    "    dated_docs.sort(key = lambda k: k['date'])\n",
    "    dates = np.asarray([[datetime.datetime.fromtimestamp(int(d['date'])).strftime('%Y-%m-%d %H:%M:%S')] for d in dated_docs])\n",
    "\n",
    "    doc_vecs = np.asarray([unpack64(t['vector']) for t in dated_docs])\n",
    "\n",
    "    results = np.dot(term_vecs, np.transpose(doc_vecs))\n",
    "    results = np.transpose(results)\n",
    "    idx = [[x] for x in list(range(0, len(results)))]\n",
    "    results = np.hstack((idx, results))\n",
    "\n",
    "    headers = ['Date','Index']\n",
    "    headers.extend(concept_list)\n",
    "\n",
    "    slopes = [linregress(results[:,x+1],results[:,0])[0] for x in range(len(results[0])-1)]\n",
    "\n",
    "    results = np.hstack((dates, results))\n",
    "    trends_table = [{key:value for key, value in zip(headers, r)} for r in results]\n",
    "    trendingterms_table = [{'Term':term, 'Slope':slope} for term, slope in zip(concept_list, slopes)]\n",
    "\n",
    "    return trends_table, trendingterms_table\n",
    "\n",
    "#def create_prediction_table():\n",
    "    \n",
    "    \n",
    "#def create_pairings_table():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_table_to_csv(table, filename):\n",
    "\n",
    "    print('Writing to file {}.'.format(filename))\n",
    "    if len(table) == 0:\n",
    "        print('Warning: No data to write to {}.'.format(filename))\n",
    "        return\n",
    "    with open(filename, 'w') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=table[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Export data to Tableau compatible CSV files.'\n",
    "    )\n",
    "    parser.add_argument('account_id', help=\"The ID of the account that owns the project, such as 'demo'\")\n",
    "    parser.add_argument('project_id', help=\"The ID of the project to analyze, such as '2jsnm'\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    client, docs, topics, terms, subsets, drivers, skt, themes = pull_lumi_data(args.account_id, args.project_id)\n",
    "\n",
    "    doc_table, xref_table = create_doc_table(client, docs, subsets, themes)\n",
    "    write_table_to_csv(doc_table, 'doc_table.csv')\n",
    "    write_table_to_csv(xref_table, 'xref_table.csv')\n",
    "\n",
    "    themes_table = create_themes_table(client, themes)\n",
    "    write_table_to_csv(themes_table, 'themes_table.csv')\n",
    "\n",
    "    skt_table = create_skt_table(client, skt)\n",
    "    write_table_to_csv(skt_table, 'skt_table.csv')\n",
    "\n",
    "    driver_table = create_drivers_table(client, drivers)\n",
    "    write_table_to_csv(driver_table, 'drivers_table.csv')\n",
    "\n",
    "    trends_table, trendingterms_table = create_trends_table(terms, topics, docs)\n",
    "    write_table_to_csv(trends_table, 'trends_table.csv')\n",
    "    write_table_to_csv(trendingterms_table, 'trendingterms_table.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
