{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from luminoso_api import LuminosoClient\n",
    "from pack64 import unpack64\n",
    "import run_voting_classifier # need accuracy/coverage chart\n",
    "from conjunctions_disjunctions import get_new_results\n",
    "from subset_key_terms import subset_key_terms\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Lumi data...\n",
      "Creating doc table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/se_code-0.1-py3.6.egg/se_code/fuzzy_logic.py:38: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file doc_table.csv.\n",
      "Writing to file xref_table.csv.\n",
      "Creating themes table...\n",
      "Writing to file themes_table.csv.\n",
      "Creating subset key terms table...\n",
      "Writing to file skt_table.csv.\n",
      "Writing to file drivers_table.csv.\n",
      "Warning: No data to write to drivers_table.csv.\n",
      "Writing to file trends_table.csv.\n",
      "Writing to file trendingterms_table.csv.\n"
     ]
    }
   ],
   "source": [
    "#%run bi_tool_export.py r85b548r prnzh4tp\n",
    "%run bi_tool_export.py d53m338v pr5cxsm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Lumi data...\n"
     ]
    }
   ],
   "source": [
    "client, docs, topics, terms, subsets, drivers, skt, themes = pull_lumi_data('d53m338v', 'pr5cxsm3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_doc_table(client, docs, subsets, themes):\n",
    "\n",
    "    print('Creating doc table...')\n",
    "    doc_table = []\n",
    "    xref_table = []\n",
    "    subset_headings = set([s['subset'].partition(':')[0] for s in subsets])\n",
    "    subset_headings = {s: i for i, s in enumerate(subset_headings)}\n",
    "    info = []\n",
    "    header = []\n",
    "    #for h,n in subset_headings.items():\n",
    "    #    header.append('Subset {}'.format(n))\n",
    "    #    info.append(h)\n",
    "    xref_table.extend([{'Header': 'Subset {}'.format(n), 'Name': h} for h,n in subset_headings.items()])\n",
    "\n",
    "    for i, theme in enumerate(themes):\n",
    "        search_terms = [t['text'] for t in theme['terms']]\n",
    "        theme['name'] = ', '.join(search_terms)[:-2]\n",
    "        theme['docs'] = get_new_results(client, search_terms, [], 'docs', 20, 'conjunction', False)\n",
    "        xref_table.append({'Header': 'Theme {}'.format(i), 'Name': theme['name']})\n",
    "        #header.append('Theme {}'.format(i))\n",
    "        #info.append(theme['name'])\n",
    "\n",
    "    for doc in docs:\n",
    "        row = {}\n",
    "        row['doc_id'] = doc['_id']\n",
    "        row['doc_text'] = doc['text']\n",
    "        if 'date' in doc:\n",
    "            row['doc_date'] = doc['date']\n",
    "        else:\n",
    "            row['doc_date'] = 0\n",
    "        row.update({'Subset {}'.format(i): '' for i in range(len(subset_headings))})\n",
    "        row.update({'Subset {}_centrality'.format(i): 0 for i in range(len(subset_headings))})\n",
    "\n",
    "        for subset in doc['subsets']:\n",
    "            subset_partition = subset.partition(':')\n",
    "            if subset_partition[0] in subset_headings:\n",
    "                row['Subset {}'.format(subset_headings[subset_partition[0]])] = subset_partition[2]\n",
    "                row['Subset {}_centrality'.format(subset_headings[subset_partition[0]])] = get_as(doc['vector'],\n",
    "                    [s['mean'] for s in subsets if s['subset'] == subset][0])\n",
    "\n",
    "        for i, theme in enumerate(themes):\n",
    "            row['Theme {}'.format(i)] = 0\n",
    "            if doc['_id'] in [d['_id'] for d in theme['docs']]:\n",
    "                row['Theme {}'.format(i)] = [d['score'] for d in theme['docs'] if d['_id'] == doc['_id']][0]\n",
    "        doc_table.append(row)\n",
    "    #xref_table = []\n",
    "    #print(header)\n",
    "    #print(info)\n",
    "    #for i in range(len(header)):\n",
    "    #    xref_table.append({header[i]:info[i]})\n",
    "    return doc_table, xref_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_doc_table(client, docs, subsets, themes):\n",
    "\n",
    "    print('Creating doc table...')\n",
    "    doc_table = []\n",
    "    xref_table = []\n",
    "    subset_headings = set([s['subset'].partition(':')[0] for s in subsets])\n",
    "    #all_index = subset_headings.index('__all__')\n",
    "    #del subset_headings[all_index]\n",
    "    subset_headings.remove('__all__')\n",
    "    subset_headings = {s: i for i, s in enumerate(subset_headings)}\n",
    "    for h,n in subset_headings.items():\n",
    "        header.append('Subset {}'.format(n))\n",
    "        info.append(h)\n",
    "   # xref_table.extend([{'Header': 'Subset {}'.format(n), 'Name': h} for h,n in subset_headings.items()])\n",
    "\n",
    "    for i, theme in enumerate(themes):\n",
    "        search_terms = [t['text'] for t in theme['terms']]\n",
    "        theme['name'] = ', '.join(search_terms)[:-2]\n",
    "        theme['docs'] = get_new_results(client, search_terms, [], 'docs', 20, 'conjunction', False)\n",
    "        #xref_table.append({'Header': 'Theme {}'.format(i), 'Name': theme['name']})\n",
    "        header.append('Theme {}'.format(i))\n",
    "        info.append(theme['name'])\n",
    "        \n",
    "        \n",
    "\n",
    "    for doc in docs:\n",
    "        row = {}\n",
    "        row['doc_id'] = doc['_id']\n",
    "        row['doc_text'] = doc['text']\n",
    "        if 'date' in doc:\n",
    "            row['doc_date'] = doc['date']\n",
    "        else:\n",
    "            row['doc_date'] = 0\n",
    "        # changed from subset # to subset (name)\n",
    "        row.update({'Subset {}'.format(i): '' for i in range(len(subset_headings))})\n",
    "        row.update({'Subset {}_centrality'.format(i): 0 for i in range(len(subset_headings))})\n",
    "\n",
    "        for subset in doc['subsets']:\n",
    "            subset_partition = subset.partition(':')\n",
    "            if subset_partition[0] in subset_headings:\n",
    "                row['Subset {}'.format(subset_headings[subset_partition[0]])] = subset_partition[2]\n",
    "                row['Subset {}_centrality'.format(subset_headings[subset_partition[0]])] = get_as(doc['vector'],\n",
    "                    [s['mean'] for s in subsets if s['subset'] == subset][0])\n",
    "\n",
    "        for i, theme in enumerate(themes):\n",
    "            row['Theme {}'.format(i)] = 0\n",
    "            if doc['_id'] in [d['_id'] for d in theme['docs']]:\n",
    "                row['Theme {}'.format(i)] = [d['score'] for d in theme['docs'] if d['_id'] == doc['_id']][0]\n",
    "        doc_table.append(row)\n",
    "    xref_table = []\n",
    "    print(header)\n",
    "    print(info)\n",
    "    for i in range(len(header)):\n",
    "        xref_table.append({header[i]:info[i]})\n",
    "    return doc_table, xref_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating doc table...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-59fa997ec763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxref_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_doc_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-205-da9ef0a5a346>\u001b[0m in \u001b[0;36mcreate_doc_table\u001b[0;34m(client, docs, subsets, themes)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdoc_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mxref_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msubset_headings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubsets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mall_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset_headings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__all__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0msubset_headings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "doc_table, xref_table = create_doc_table(client, docs, subsets, themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Header': 'Subset 0', 'Name': '__all__'}, {'Header': 'Subset 1', 'Name': 'Locale'}, {'Header': 'Subset 2', 'Name': 'Product'}, {'Header': 'Subset 3', 'Name': 'Subject'}, {'Header': 'Theme 0', 'Name': 'PC, purchase, blizzard, play Overwat'}, {'Header': 'Theme 1', 'Name': 'frustrating, annoying, realized, unfa'}, {'Header': 'Theme 2', 'Name': 'theres, appreciate, Orisa, cli'}, {'Header': 'Theme 3', 'Name': 'crashes, competitive, SR, competitive mat'}, {'Header': 'Theme 4', 'Name': 'voice chat, chat, mic, silenc'}, {'Header': 'Theme 5', 'Name': 'reset, ping, switch, rout'}, {'Header': 'Theme 6', 'Name': 'settings, error, tried, install'}]\n"
     ]
    }
   ],
   "source": [
    "print(xref_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Subset 0': '__all__'}, {'Subset 1': 'Locale'}, {'Subset 2': 'Product'}, {'Subset 3': 'Subject'}, {'Theme 0': 'PC, purchase, blizzard, play Overwat'}, {'Theme 1': 'frustrating, annoying, realized, unfa'}, {'Theme 2': 'theres, appreciate, Orisa, cli'}, {'Theme 3': 'crashes, competitive, SR, competitive mat'}, {'Theme 4': 'voice chat, chat, mic, silenc'}, {'Theme 5': 'reset, ping, switch, rout'}, {'Theme 6': 'settings, error, tried, install'}]\n"
     ]
    }
   ],
   "source": [
    "print(xref_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_trends_table(terms, topics, docs):\n",
    "    term_vecs = np.asarray([unpack64(t['vector']) for t in terms])\n",
    "    concept_list = [t['text'] for t in terms]\n",
    "\n",
    "    dated_docs = [d for d in docs if 'date' in d]\n",
    "    dated_docs.sort(key = lambda k: k['date'])\n",
    "    dates = np.asarray([[datetime.datetime.fromtimestamp(int(d['date'])).strftime('%Y-%m-%d %H:%M:%S')] for d in dated_docs])\n",
    "\n",
    "    doc_vecs = np.asarray([unpack64(t['vector']) for t in dated_docs])\n",
    "    results = np.dot(term_vecs, np.transpose(doc_vecs))\n",
    "    results = np.transpose(results)\n",
    "    idx = [[x] for x in range(0, len(results))]\n",
    "    results = np.hstack((idx, results))\n",
    "    \n",
    "    headers = ['Date','Index']\n",
    "    headers.extend(concept_list)\n",
    "    \n",
    "    tenth = int(.9 * len(results))\n",
    "    quarter = int(.75 * len(results))\n",
    "    half = int(.5 * len(results))\n",
    "\n",
    "    slopes = [linregress(results[:,x+1],results[:,0])[0] for x in range(len(results[0])-1)]\n",
    "    slope_ranking = zip(concept_list, slopes)\n",
    "    slope_ranking = sorted(slope_ranking, key=lambda rank:rank[1])\n",
    "    slope_ranking = slope_ranking[::-1]\n",
    "    \n",
    "    tenth_slopes = [linregress(results[tenth:,x+1],results[tenth:,0])[0] for x in range(len(results[0]) - 1)]\n",
    "    tenth_slope_ranking = zip(concept_list, tenth_slopes)\n",
    "    tenth_slope_ranking = sorted(tenth_slope_ranking, key=lambda rank:rank[1])\n",
    "    tenth_slope_ranking = tenth_slope_ranking[::-1]\n",
    "    \n",
    "    quarter_slopes = [linregress(results[quarter:,x+1],results[quarter:,0])[0] for x in range(len(results[0]) - 1)]\n",
    "    quarter_slope_ranking = zip(concept_list, quarter_slopes)\n",
    "    quarter_slope_ranking = sorted(quarter_slope_ranking, key=lambda rank:rank[1])\n",
    "    quarter_slope_ranking = quarter_slope_ranking[::-1]\n",
    "    \n",
    "    half_slopes = [linregress(results[half:,x+1],results[half:,0])[0] for x in range(len(results[0]) - 1)]\n",
    "    half_slope_ranking = zip(concept_list, half_slopes)\n",
    "    half_slope_ranking = sorted(half_slope_ranking, key=lambda rank:rank[1])\n",
    "    half_slope_ranking = half_slope_ranking[::-1]\n",
    "    \n",
    "    results = np.hstack((dates, results))\n",
    "    trends_table = [{key:value for key, value in zip(headers, r)} for r in results]\n",
    "    trendingterms_table = [{'Term':term, \n",
    "                            'Slope':slope, \n",
    "                            'Rank':slope_ranking.index((term, slope)), \n",
    "                            'Short term slope':tenth_slope, \n",
    "                            'Short term rank':tenth_slope_ranking.index((term, tenth_slope)), \n",
    "                            'Medium term slope':quarter_slope,\n",
    "                            'Medium term rank':quarter_slope_ranking.index((term, quarter_slope)), \n",
    "                            'Half term slope':half_slope, \n",
    "                            'Half term rank':half_slope_ranking.index((term, half_slope))}\n",
    "                           for term, slope, tenth_slope, quarter_slope, half_slope in zip(concept_list, slopes, tenth_slopes, quarter_slopes, half_slopes)]\n",
    "\n",
    "    return trends_table, trendingterms_table, results\n",
    "\n",
    "#def create_prediction_table():\n",
    "    \n",
    "    \n",
    "#def create_pairings_table():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_table, trendingterms_table, results = create_trends_table(terms, topics, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Term': 'Overwatch', 'Slope': -12.415890718576328, 'Rank': 561, 'Short term slope': 4.8183092712083946, 'Short term rank': 461, 'Medium term slope': 34.72424232637988, 'Medium term rank': 176, 'Half term slope': -39.773269532714089, 'Half term rank': 816}\n"
     ]
    }
   ],
   "source": [
    "print(trendingterms_table[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_drivers_table(client, drivers):\n",
    "    driver_table = []\n",
    "    for subset in drivers:\n",
    "        score_drivers = client.get('prediction/drivers', predictor_name=subset)\n",
    "        for driver in score_drivers['negative']:\n",
    "            row = {}\n",
    "            row['driver'] = driver['text']\n",
    "            row['subset'] = subset\n",
    "            row['impact'] = driver['regressor_dot']\n",
    "            row['score'] = driver['driver_score']\n",
    "            # ADDED RELATED TERMS\n",
    "            related_terms = driver['similar_terms']\n",
    "            row['related_terms'] = related_terms\n",
    "            doc_count = client.get('terms/doc_counts', terms=related_terms, use_json=True)\n",
    "            count_sum = 0\n",
    "            for doc_dict in doc_count:\n",
    "                count_sum += (doc_dict['num_related_matches'] + doc_dict['num_exact_matches'])\n",
    "            row['doc_count'] = count_sum\n",
    "            \n",
    "    \n",
    "            # Use the driver term to find related documents\n",
    "            search_docs = client.get('docs/search', terms=[driver['term']], limit=500, match_type='exact')\n",
    "    \n",
    "            # Sort documents based on their association with the coefficient vector\n",
    "            for doc in search_docs['search_results']:\n",
    "                document = doc[0]['document']\n",
    "                document['driver_as'] = get_as(score_drivers['coefficient_vector'],document['vector'])\n",
    "\n",
    "            docs = sorted(search_docs['search_results'], key=lambda k: k[0]['document']['driver_as'])\n",
    "            row['example_doc'] = docs[0][0]['document']['text']\n",
    "            driver_table.append(row)\n",
    "        for driver in score_drivers['positive']:\n",
    "            row = {}\n",
    "            row['driver'] = driver['text']\n",
    "            row['subset'] = subset\n",
    "            row['impact'] = driver['regressor_dot']\n",
    "            row['score'] = driver['driver_score']\n",
    "            related_terms = driver['similar_terms']\n",
    "            row['related_terms'] = related_terms\n",
    "            doc_count = client.get('terms/doc_counts', terms=related_terms, use_json=True)\n",
    "            count_sum = 0\n",
    "            for doc_dict in doc_count:\n",
    "                count_sum += (doc_dict['num_related_matches'] + doc_dict['num_exact_matches'])\n",
    "            row['doc_count'] = count_sum\n",
    "\n",
    "            # Use the driver term to find related documents\n",
    "            search_docs = client.get('docs/search', terms=[driver['term']], limit=500, match_type='exact')\n",
    "\n",
    "            # Sort documents based on their association with the coefficient vector\n",
    "            for doc in search_docs['search_results']:\n",
    "                document = doc[0]['document']\n",
    "                document['driver_as'] = get_as(score_drivers['coefficient_vector'],document['vector'])\n",
    "\n",
    "            docs = sorted(search_docs['search_results'], key=lambda k: -k[0]['document']['driver_as'])\n",
    "            row['example_doc'] = docs[0][0]['document']['text']\n",
    "            driver_table.append(row)\n",
    "    \n",
    "    return driver_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_as(vector1, vector2):\n",
    "    return np.dot(unpack64(vector1), unpack64(vector2))\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_lumi_data(account, project, term_count=1000, interval='day', themes=7, theme_terms=4):\n",
    "\n",
    "    print('Extracting Lumi data...')\n",
    "    client = LuminosoClient.connect('/projects/{}/{}'.format(account, project))\n",
    "    subsets = client.get('subsets/stats')\n",
    "\n",
    "    docs = []\n",
    "    while True:\n",
    "        new_docs = client.get('docs', limit=25000, offset=len(docs))\n",
    "        if new_docs:\n",
    "            docs.extend(new_docs)\n",
    "        else:\n",
    "            break\n",
    "    drivers = list(set([key for d in docs for key in d['predict'].keys()]))\n",
    "\n",
    "    # See if any score drivers are present, if not, create some from subsets\n",
    "    if not any(drivers):\n",
    "        drivers = []\n",
    "        subset_headings = list(set([s['subset'].partition(':')[0] for s in subsets]))\n",
    "        for subset in subset_headings:\n",
    "            subset_values = [s['subset'].partition(':')[2] for s in subsets\n",
    "                             if s['subset'].partition(':')[0] == subset]\n",
    "            if all([is_number(v) for v in subset_values]):\n",
    "                drivers.append(subset)\n",
    "        if drivers:\n",
    "            add_score_drivers_to_project(client, docs, drivers)\n",
    "\n",
    "    topics = client.get('topics')\n",
    "    themes = client.get('/terms/clusters/', num_clusters=themes, num_cluster_terms=theme_terms)\n",
    "    terms = client.get('terms', limit=term_count)\n",
    "    terms_doc_count = client.get('terms/doc_counts', limit=term_count, format='json')\n",
    "    skt = subset_key_terms(client, 20)\n",
    "    return client, docs, topics, terms, subsets, drivers, skt, themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_skt_table(client, skt):\n",
    "\n",
    "    print('Creating subset key terms table...')\n",
    "    terms = client.get('terms/doc_counts',\n",
    "                       terms=[t['term'] for _, t, _, _ in skt],\n",
    "                       format='json')\n",
    "    terms = {t['text']: t for t in terms}\n",
    "    skt_table = [{'term': t['text'],\n",
    "                  'subset': s.partition(':')[0],\n",
    "                  'value': s.partition(':')[2],\n",
    "                  'odds_ratio': o,\n",
    "                  'p_value': p,\n",
    "                  'exact_matches': terms[t['text']]['num_exact_matches'],\n",
    "                  'conceptual_matches': terms[t['text']]['num_related_matches']}\n",
    "                 for s, t, o, p in skt]\n",
    "    return skt_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_score_drivers_to_project(client, docs, drivers):\n",
    "    mod_docs = []\n",
    "    for doc in docs:\n",
    "        for subset_to_score in drivers:\n",
    "            if subset_to_score in [a.split(':')[0] for a in doc['subsets']]:\n",
    "                mod_docs.append({'_id': doc['_id'],\n",
    "                                 'predict': {subset_to_score: float([a for a in doc['subsets'] \n",
    "                                    if subset_to_score in a][0].split(':')[1])}})\n",
    "    client.put_data('docs', json.dumps(mod_docs), content_type='application/json')\n",
    "    client.post('docs/recalculate')\n",
    "\n",
    "    time_waiting = 0\n",
    "    while True:\n",
    "        if time_waiting%30 == 0:\n",
    "            if len(client.get()['running_jobs']) == 0:\n",
    "                break\n",
    "        sys.stderr.write('\\r\\tWaiting for recalculation ({}sec)'.format(time_waiting))\n",
    "        time.sleep(30)\n",
    "        time_waiting += 30\n",
    "    print('Done recalculating. Training...')\n",
    "    client.post('prediction/train')\n",
    "    print('Done training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_themes_table(client, themes):\n",
    "    print('Creating themes table...')\n",
    "    for i, theme in enumerate(themes):\n",
    "        search_terms = [t['text'] for t in theme['terms']]\n",
    "        theme['name'] = ', '.join(search_terms)\n",
    "        theme['id'] = i\n",
    "        theme['docs'] = sum([t['distinct_doc_count'] for t in theme['terms']])\n",
    "        del theme['terms']\n",
    "    return themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_table_to_csv(table, filename):\n",
    "\n",
    "    print('Writing to file {}.'.format(filename))\n",
    "    if len(table) == 0:\n",
    "        print('Warning: No data to write to {}.'.format(filename))\n",
    "        return\n",
    "    with open(filename, 'w') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=table[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Export data to Business Intelligence compatible CSV files.'\n",
    "    )\n",
    "    parser.add_argument('account_id', help=\"The ID of the account that owns the project, such as 'demo'\")\n",
    "    parser.add_argument('project_id', help=\"The ID of the project to analyze, such as '2jsnm'\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    client, docs, topics, terms, subsets, drivers, skt, themes = pull_lumi_data(args.account_id, args.project_id)\n",
    "\n",
    "    doc_table, xref_table = create_doc_table(client, docs, subsets, themes)\n",
    "    write_table_to_csv(doc_table, 'doc_table.csv')\n",
    "    write_table_to_csv(xref_table, 'xref_table.csv')\n",
    "\n",
    "    themes_table = create_themes_table(client, themes)\n",
    "    write_table_to_csv(themes_table, 'themes_table.csv')\n",
    "\n",
    "    skt_table = create_skt_table(client, skt)\n",
    "    write_table_to_csv(skt_table, 'skt_table.csv')\n",
    "\n",
    "    driver_table = create_drivers_table(client, drivers)\n",
    "    write_table_to_csv(driver_table, 'drivers_table.csv')\n",
    "\n",
    "    trends_table, trendingterms_table = create_trends_table(terms, topics, docs)\n",
    "    write_table_to_csv(trends_table, 'trends_table.csv')\n",
    "    write_table_to_csv(trendingterms_table, 'trendingterms_table.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}