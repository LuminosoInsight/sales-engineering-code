{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for Topic Expansion  & Topic Docs Correlations (version v5b)\n",
    "## Part A:  Auto-Topic Expansion, with optional stats on subsets\n",
    "## Part B:  Update Topics in Project & get Topics-Docs correlations (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boaz Odier, bodier@luminoso.com - v5b: with all parameters at the top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The script here also takes in topics definitions from a CSV file, and saves those topics onto the given project.\n",
    "The format of the topic input file (topics_file = 'PA_Default_Topics.csv' in the code below) should be as follows, in csv format:\n",
    "\n",
    "Topic Name,Topic Terms\n",
    "DESIGN1,\"design, style, color, red\"\n",
    "DESIGN2,\"classic, elegant, simple, beautiful, fashion\"\n",
    "MATERIAL1,\"leather, primeknit\"\n",
    "MATERIAL2,\"soft\"\n",
    "SERVICE,\"refund, delivery, package, customer, service\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for topic expansion:\n",
    "ASSOCIATION_THRESHOLD = .6  #only terms with association score above this will appear\n",
    "chosen_subset = 'Franchise' #if you want to run the topic expans on a specific subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the filename, from wich the topics will be loaded - must be in same folder as this file\n",
    "topics_file = 'PA_Default_Topics.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Options for Topic-Correlations export (Classfication)\n",
    "USE_TOPIC_THRESHOLD = True\n",
    "TOPIC_THRESHOLD = .35  #the threshold over which a document is classfied as inside a Topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Luminoso Project on which to run the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luminoso_api import LuminosoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sconnection parameters\n",
    "user_name = 'bodier@luminoso.com'  #change to your own email that you use to login to Luminoso\n",
    "\n",
    "#use the correct one from below:\n",
    "api_url = 'https://eu-analytics.luminoso.com/api/v4' \n",
    "#api_url = 'https://analytics.luminoso.com/api/v4' \n",
    "\n",
    "#adidas Product Analytics: s75r663v  \n",
    "#adidas Training acount:  u46p858s\n",
    "account_id = 'u46p858s' #this is the accountID for your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password for bodier@luminoso.com: ········\n",
      "Connected to project: Copy of Football Reviews [On workspace: Adidas: Training]\n"
     ]
    }
   ],
   "source": [
    "project_id = 'pr97kgx5'  \n",
    "\n",
    "#Connect to that specific project\n",
    "account_url = api_url + '/projects/' + account_id + '/'\n",
    "\n",
    "client = LuminosoClient.connect(account_url + '/' + project_id, username=user_name)\n",
    "project_info =  client.get()\n",
    "proj_name = project_info['name']\n",
    "print('Connected to project: ' +  proj_name, '[On workspace: ' + project_info['account_name'] +']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for CSV Input/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_to_CSV(filename, data):\n",
    "    if len(filename) < 4 or filename[-4:] != '.csv':\n",
    "        filename += '.csv'\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "    print('Data saved to file:  ', filename)\n",
    "\n",
    "def load_from_CSV(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = [row for row in csv.DictReader(file)]\n",
    "    print('Data loaded from file:  ', filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions specific to Luminoso projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#may want to refactor this so output of 2nd value is a dict with key = subsettype, value = list of possible values\n",
    "#then the get_subset_elements function can be subsumed.  \n",
    "def get_subsets_info(connection):\n",
    "    '''\n",
    "    Given a Luminoso project connection, this will simply give the 'subsets/stats' API endpoint info,\n",
    "    but also as a second value a list of the Subset Types. \n",
    "    '''\n",
    "    substats = connection.get('subsets/stats')\n",
    "    type_list = []\n",
    "    for s in substats:\n",
    "        if s['subset'] == '__all__':\n",
    "            t = '__all__'\n",
    "            s['subset_type'] = t\n",
    "            s['subset_element'] = ''\n",
    "        else:\n",
    "            (t, e) = s['subset'].split(': ')\n",
    "            s['subset_type'] = t\n",
    "            s['subset_element'] = e\n",
    "        \n",
    "        if t not in type_list: \n",
    "            type_list.append(t)\n",
    "    \n",
    "    return substats, type_list\n",
    "\n",
    "def get_subset_elements(subsets_info, subset_type):\n",
    "    results = []\n",
    "    for s in subsets_info:\n",
    "        if s['subset_type'] == subset_type:\n",
    "            results.append(s['subset'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_docs(client, doc_fields=None):\n",
    "    '''\n",
    "    Get all docs\n",
    "    '''\n",
    "    docs = []\n",
    "    while True:\n",
    "        if doc_fields:\n",
    "            newdocs = client.get('docs', limit=25000, offset=len(docs), doc_fields=doc_fields)\n",
    "        else:\n",
    "            newdocs = client.get('docs', limit=25000, offset=len(docs))\n",
    "        if newdocs:\n",
    "            docs.extend(newdocs)\n",
    "        else:\n",
    "            return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_array_to_dict(subsets_array):\n",
    "    '''\n",
    "    Given an array of subset values (as per Luminoso output from docs download),\n",
    "    transforms it into a dictionary format\n",
    "    '''\n",
    "    obj={}\n",
    "    for s in subsets_array:\n",
    "        if  s != '__all__':\n",
    "            (sub, val) = s.split(': ')\n",
    "            obj[sub] = val\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_keys_in_dict(data_dict, new_keys_dict):\n",
    "    '''\n",
    "    Substitute the keys used in the data_dict, to the values returned by the new_keys_dict\n",
    "    '''\n",
    "    result = {}\n",
    "    for k, v in data_dict.items():\n",
    "        result[new_keys_dict[k]] = v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ID_TOPICNAME_dict(topicsdata):\n",
    "    '''\n",
    "    Given a Luminoso list of topics, as per API call to get('topics'), \n",
    "    build a topic ID to NAME mapping table (as a dict)\n",
    "    '''\n",
    "    result = {}\n",
    "    for t in topicsdata:\n",
    "        result[t['_id']] = t['name']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_update_topics(connection, new_topics):\n",
    "    '''\n",
    "    This function will create the given new topics into the project\n",
    "    If a topic already exists (by name), then it will be overwritten with the new terms given\n",
    "    Returns a dict mapping table of topicIDs to TopicNames\n",
    "    '''\n",
    "    project_info = connection.get()\n",
    "    existing_topics = connection.get('topics')\n",
    "    \n",
    "    for n in new_topics:\n",
    "        newName = n['Topic Name']\n",
    "        newTerms = n['Topic Terms']\n",
    "        isnew = True\n",
    "        for x in existing_topics:\n",
    "            if newName == x['name']:\n",
    "                connection.put('topics/id/' + x['_id'], text = newTerms, name = newName) #we have to give the name as parameter as well, if not the name defaults to list of terms\n",
    "                isnew = False\n",
    "                print('The topic', newName, 'already exists - Overwritten with: ', newTerms )\n",
    "        if isnew:\n",
    "            connection.post('topics', text = newTerms, name = newName)\n",
    "            print('The topic', newName, 'is new - Created with', newTerms)\n",
    "    \n",
    "    print('\\nThe topics have been updated for project:',  project_info['name'], ', on workspace:', project_info['account_name'] )\n",
    "    \n",
    "    return connection.get('topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_topics(connection):\n",
    "    '''\n",
    "    This function will delete al the topics (saved concepts) in a given project\n",
    "    This is not reversible, so the output of the function are all the topics before deletion.\n",
    "    '''\n",
    "    project_info = connection.get()\n",
    "    existing_topics = connection.get('topics')\n",
    "    all_deleted = True\n",
    "    \n",
    "    for t in existing_topics:\n",
    "        t_name = t['name']\n",
    "        t_id = t['_id']\n",
    "        result = client.delete('topics/id/' + t_id)\n",
    "        if 'deleted' not in result.keys():\n",
    "            print('Warning: The Topic: ', t_name, ' could not be deleted.')\n",
    "            all_deleted = False\n",
    "\n",
    "    if all_deleted:\n",
    "        print('\\nAll Topics have been DELETED for Project:',  project_info['name'], ', on workspace:', project_info['account_name'] )\n",
    "    \n",
    "    return existing_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Topics from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from file:   PA_Default_Topics_v3_[Example with Sentiment].csv\n",
      "\n",
      "DESIGN1 : design, style, color, red\n",
      "DESIGN2 : classic, elegant, simple, beautiful, fashion\n",
      "MATERIAL1 : leather, primeknit\n",
      "MATERIAL2 : soft\n",
      "MATERIAL3 : boost, cloudfoam, bounce\n",
      "CONSTRUCTION1 : tongue, heel, midsole, sole, toe\n",
      "CONSTRUCTION2 : quality\n",
      "CONSTRUCTION3 : shape, form\n",
      "FIT1 : narrow, small, size, width, fit, wide, large\n",
      "FIT2 : hurt, rubs\n",
      "PRICE : money, price, worth, sale, cheap\n",
      "FUNCTION1 : office, everyday, casual\n",
      "FUNCTION2 : run, walk, activity, gym\n",
      "WEBSITE : image, site, description, online\n",
      "SERVICE : refund, delivery, package, customer, service\n",
      "SENTIMENT_NEGATIVE : bad,uncomfortable,terrible,ridiculous,cringe,awful,sadly,annoying,frustration,disgusted,upset\n",
      "SENTIMENT_POSITIVE : great,good,awesome,super,dope,comfortable,happy,nice,cool,excellent,fantastic,pleasant\n"
     ]
    }
   ],
   "source": [
    "# Load all topics from the CSV Topic File\n",
    "input_topics = load_from_CSV(topics_file)\n",
    "\n",
    "print('')\n",
    "for x in input_topics:\n",
    "    print(x['Topic Name'], ':' ,  x['Topic Terms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A: Topic Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand a topic list. \n",
    "# A connection to a project is needed, and optionally the subset to provide stats upon.\n",
    "\n",
    "def expand_topic_list(connection, subfilter='__all__'):\n",
    "\n",
    "    #default subfilter, if not given, is __all__ , which is the same as not giving any subfilter. \n",
    "\n",
    "    #get the subsets info and check the provided subfilter actually exists:\n",
    "    project_subsets, project_subsetTypes = get_subsets_info(connection)\n",
    "    \n",
    "    if subfilter not in project_subsetTypes:\n",
    "        print('Error: subfilter provided does not exist in Project')\n",
    "        return 'error'\n",
    "\n",
    "    subset_list = get_subset_elements(project_subsets, subfilter)\n",
    "    \n",
    "    #an empty array to fill with results \n",
    "    output_list = [] \n",
    "\n",
    "    #iterate through each master topic and its given associated list of terms.\n",
    "    for item in input_topics: \n",
    "\n",
    "        input_topic = item['Topic Name']\n",
    "        input_list = item['Topic Terms']\n",
    "\n",
    "        # collect only the 'term' value from search results, with a high enough score threshold\n",
    "        # this are the new expanded topic terms\n",
    "        new_topic_terms_scores = { \n",
    "                    t['text']:[t['term'], s]  #for each text, give the term(s) & the score\n",
    "                    for t,s in client.get('terms/search', text = input_list, limit = 100)['search_results']\n",
    "                    if s > ASSOCIATION_THRESHOLD}\n",
    "        \n",
    "        new_topic_terms = [ t for t,s in new_topic_terms_scores.values() ]\n",
    "\n",
    "        #iterate over each subset to get the counts:\n",
    "        for sl in subset_list:\n",
    "            \n",
    "            #collect counting stats on each of the new terms\n",
    "            topic_counts = client.get('terms/doc_counts', terms = new_topic_terms, subset = sl, format='json')\n",
    "            \n",
    "            if sl == '__all__':\n",
    "                subset_type = ''\n",
    "                subset_element = '__all__'\n",
    "            else:\n",
    "                (subset_type, subset_element) = sl.split(': ')\n",
    "\n",
    "            for topic in topic_counts:\n",
    "                topic['master_topic'] = input_topic  #add a label referencing the current main topic\n",
    "                topic['new_topic'] = '{}'.format(topic['text'] not in input_list) #add label with True/False value\n",
    "                topic['assoc_score'] = new_topic_terms_scores[topic['text']][1] #get the score using our data above\n",
    "                topic['Subset_' + subset_type] = subset_element\n",
    "                \n",
    "            # Add all the results into our output_list variable (final results)\n",
    "            output_list.extend(topic_counts)\n",
    "            \n",
    "            ## The remainder of the code is to find the Total on the input master topic itself. \n",
    "\n",
    "            #we put the input_list in a JSON-encoded array of strings [{}], \n",
    "            #and send it to 'docs/vectors' API endpoint\n",
    "            master_topic_terms = client.upload('docs/vectors', [{'text': input_list }] )[0]['terms']\n",
    "            master_topic_terms = [t for t,_,_ in master_topic_terms]\n",
    "\n",
    "            #now we can use the docs/search endpoint to directly get the TOTAL COUNT on the original list of terms\n",
    "            master_counts = client.get('docs/search', terms = master_topic_terms, limit = 1, subset = sl)\n",
    "\n",
    "            #so we can now add to our output an extra line with total matching docs count for the master topic:\n",
    "            output_list.append({'master_topic': input_topic,\n",
    "                                'text': input_topic, \n",
    "                                'new_topic': 'False',\n",
    "                                'assoc_score': 1,\n",
    "                                'num_related_matches': master_counts['num_related_matches'],  #total for input list of terms\n",
    "                                'num_exact_matches': master_counts['num_exact_matches'],      #total for input list of terms\n",
    "                                'Subset_' + subset_type: subset_element})\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Topic Expansion without subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with no subset\n",
    "output_TopExpNoSubsets = expand_topic_list(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to file:   Copy of Football Reviews_TopExp_(NoSubsets).csv\n"
     ]
    }
   ],
   "source": [
    "outputfile_TopExpNS = proj_name + '_TopExp_(NoSubsets).csv'\n",
    "save_to_CSV(outputfile_TopExpNS, output_TopExpNoSubsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Topic Expansion with a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Country', 'Rating', 'Franchise', '__all__']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the list of subsets available (metadata)\n",
    "project_subsets, project_subsetTypes = get_subsets_info(client)\n",
    "project_subsetTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Franchise: ZX',\n",
       " 'Franchise: Predator',\n",
       " 'Franchise: X',\n",
       " 'Franchise: not applicable',\n",
       " 'Franchise: MESSI',\n",
       " 'Franchise: COPA',\n",
       " 'Franchise: NEMEZIZ',\n",
       " 'Franchise: Tubular']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line is to just illustrate what's in the Franchise Subset\n",
    "exple_subset = get_subset_elements(project_subsets, chosen_subset)\n",
    "exple_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run with the chosen Subset:\n",
    "output_TopExpWithSubset = expand_topic_list(client, subfilter=chosen_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to file:   Copy of Football Reviews_TopExp_(WithSubset_Franchise).csv\n"
     ]
    }
   ],
   "source": [
    "outputfile_TopExpWiSub = proj_name + '_TopExp_(WithSubset_' + chosen_subset  + ').csv'\n",
    "save_to_CSV(outputfile_TopExpWiSub, output_TopExpWithSubset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B: Topics-Docs Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3691 documents loaded from project: Copy of Football Reviews\n"
     ]
    }
   ],
   "source": [
    "#Download all documents from the Project\n",
    "DOCS = get_all_docs(client)\n",
    "print(len(DOCS), 'documents loaded from project:', client.get()['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic DESIGN1 already exists - Overwritten with:  design, style, color, red\n",
      "The topic DESIGN2 already exists - Overwritten with:  classic, elegant, simple, beautiful, fashion\n",
      "The topic MATERIAL1 already exists - Overwritten with:  leather, primeknit\n",
      "The topic MATERIAL2 already exists - Overwritten with:  soft\n",
      "The topic MATERIAL3 already exists - Overwritten with:  boost, cloudfoam, bounce\n",
      "The topic CONSTRUCTION1 already exists - Overwritten with:  tongue, heel, midsole, sole, toe\n",
      "The topic CONSTRUCTION2 already exists - Overwritten with:  quality\n",
      "The topic CONSTRUCTION3 already exists - Overwritten with:  shape, form\n",
      "The topic FIT1 already exists - Overwritten with:  narrow, small, size, width, fit, wide, large\n",
      "The topic FIT2 already exists - Overwritten with:  hurt, rubs\n",
      "The topic PRICE already exists - Overwritten with:  money, price, worth, sale, cheap\n",
      "The topic FUNCTION1 already exists - Overwritten with:  office, everyday, casual\n",
      "The topic FUNCTION2 already exists - Overwritten with:  run, walk, activity, gym\n",
      "The topic WEBSITE already exists - Overwritten with:  image, site, description, online\n",
      "The topic SERVICE already exists - Overwritten with:  refund, delivery, package, customer, service\n",
      "The topic SENTIMENT_NEGATIVE already exists - Overwritten with:  bad,uncomfortable,terrible,ridiculous,cringe,awful,sadly,annoying,frustration,disgusted,upset\n",
      "The topic SENTIMENT_POSITIVE already exists - Overwritten with:  great,good,awesome,super,dope,comfortable,happy,nice,cool,excellent,fantastic,pleasant\n",
      "\n",
      "The topics have been updated for project: Copy of Football Reviews , on workspace: Adidas: Training\n"
     ]
    }
   ],
   "source": [
    "#Upload all new topics onto the Project, overwrite if they exist\n",
    "refreshed_topics = create_or_update_topics(client, input_topics)\n",
    "\n",
    "#use the helper function to get a mapping table from topic IDs to their Names\n",
    "id_topicnames_table = build_ID_TOPICNAME_dict(refreshed_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each document, this gives the topics-document correlations\n",
    "docs_topics_correl = client.get('docs/correlations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which info to keep for export, and add the corresponding correlations to topics\n",
    "\n",
    "docs_output = []\n",
    "for d in DOCS:\n",
    "    obj = {}\n",
    "    obj['Luminoso_docID'] = d['_id']\n",
    "    obj['text'] = d['text']\n",
    "    obj['title'] = d['title']\n",
    "    metadata = subset_array_to_dict(d['subsets'])\n",
    "    for subset, value in metadata.items():\n",
    "        obj['Subset_' + subset] = value\n",
    "    topics_correl = docs_topics_correl[d['_id']]\n",
    "    topics_correl = substitute_keys_in_dict(topics_correl, id_topicnames_table)\n",
    "    #add correlation scores for each topic:\n",
    "    for topic, correl in topics_correl.items():\n",
    "        obj['Topic_' + topic] = correl\n",
    "        # optionally add a YesNo field based on threshold parameter:\n",
    "        if USE_TOPIC_THRESHOLD:\n",
    "            if correl >= TOPIC_THRESHOLD:\n",
    "                obj['Topic_' + topic + '_YesNo'] = 1\n",
    "            else:\n",
    "                obj['Topic_' + topic + '_YesNo'] = 0\n",
    "    docs_output.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to file:   Copy of Football Reviews_DocTopCorrel.csv\n"
     ]
    }
   ],
   "source": [
    "# Write to Output File\n",
    "docs_output_file = proj_name + '_DocTopCorrel' + '.csv'\n",
    "save_to_CSV(docs_output_file, docs_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
